# Splunk-GABQ-Addon

This addon includes an input script so users of Google Analytics Premium can connect Splunk to their GAP account.
It it aimed primarily at users who maintain or monitor multiple websites with multiple property IDs.

Prerequisites:
* Google Analytics Premium
* A View ID of the GAP account you want to use - preferably one that does not have filtering
* A Google Cloud Platform project
* GAP connected to BigQuery (see details on Google's site: https://support.google.com/analytics/answer/3437618?hl=en)
  * Note that this is unlikely to cost anything for anything other than the most busy sites as, at time of writing, google give a $500/pm credit for cloud platform.
* You need your client ID and secret for the project, and rights from a user account to the BQ data.

Setting up:
* Install the plugin on the Splunk server.
* Run the "get_tokens.py" script on your splunk server. 
    * You need the project client ID and secret
    * Copy the URL into a browser, login as an account that can read the data from your BQ project, and approve the request
    * Paste the code back into the script.
* Set up a new GABQ input. Plug in all the relevant details. the access and refresh tokens come from the output of the get_tokens script.
* You nearly definately want to push the data into a new index.
* Start the input.

What you get:
* Two sourcetypes, ga_sessions and ga_hits
ga_sessions are one record per session started, and include the global session details like device, geoNetwork, trafficSource, etc as well as hitCount (total amount of hits for that session) and page_hostname which is the hostname of the site being hit (not always the same as the intended website).

ga_hits is the details of each individual hit, including the javascript events generated by the page.

Refer to Google's documentation for a full description of the data types: https://support.google.com/analytics/answer/3437719?hl=en

In future revisions the data structure might eliminate the hitCount item. The input does not currently support the "intraday" mechanism, so for in progress datasets you'd need to work it out. For complete datasets I may include a totals sourcetype to handle this, just containing the totals* information from the data.

Notes:
* Google do a bulk data load for GAP accounts when they initially set up the BQ transfer, roughly 12-13 months of data. This can take some time to process.
* Monitor the progress of the input with the command "index=_internal sourcetype=splunkd gabq"

The app is generally quite chatty and informs you of when tables are started, stopped, how many records were loaded, etc. If you receive an error for any reason, you can run the command above which will show the entire output of the failed event.

Normal debug events generated by the input look like this:

Start table ingest=dto-analytics:71601106.ga_sessions_20140827

Finished table ingest=dto-analytics:71601106.ga_sessions_20140826 chunkcount=28 hitcount=139949 sessioncount=81489

hitcount and sessioncount are how many hits and sessions were in that day's logs, and chunkcount gives you an idea of how large the data transfer was - each chunk is around 20Mb. On the test site with roughly 140-200k hits / 55-80k sessions per day, the data size was about 500Mb. 

An input created with dataset=* will consume any gap dataset it finds without discrimination, and will continue until there are none left.

If, for some reason, you need to reload a day's logs, you need to know the table name, dataset name (same as the viewID) and project name. Once you do, run "index=<whatever> source=projectname:datasetname.tablename | delete". On the next pass the input will collect the data again.
